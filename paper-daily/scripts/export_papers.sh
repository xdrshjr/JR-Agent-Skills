#!/bin/bash
#
# paper-daily: Export papers to document format with GitHub screenshots
# Usage: ./export_papers.sh [markdown|word] [count]

FORMAT=${1:-word}
COUNT=${2:-10}
OUTPUT_DIR="$HOME/clawd"
DATE_STR=$(date '+%Y-%m-%d')
TEMP_DIR="/tmp/paper_daily_$$"

mkdir -p "$TEMP_DIR"

echo "ğŸ“š æ­£åœ¨è·å– ${COUNT} ç¯‡è®ºæ–‡..."
echo ""

# è·å–è®ºæ–‡åˆ—è¡¨
PAPERS=$(curl -s "https://arxiv.org/list/cs.AI/recent" | grep -o 'arXiv:[0-9.]\+' | head -$COUNT | sed 's/arXiv://')

if [ -z "$PAPERS" ]; then
    echo "âŒ è·å–è®ºæ–‡åˆ—è¡¨å¤±è´¥"
    rm -rf "$TEMP_DIR"
    exit 1
fi

# å¦‚æœæ˜¯wordæ ¼å¼ï¼Œä½¿ç”¨Pythonç”Ÿæˆ
if [ "$FORMAT" = "word" ]; then
    echo "ğŸ“ æ­£åœ¨ç”Ÿæˆ Word æ–‡æ¡£ï¼ˆåŒ…å«GitHubæˆªå›¾ï¼‰..."
    
    python3 << PYTHON_EOF
import subprocess
import re
import os
import json
from docx import Document
from docx.shared import Pt, RGBColor, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn
from datetime import datetime

def fetch_github_screenshots(arxiv_id):
    """ä»è®ºæ–‡ä¸­æå–GitHubé“¾æ¥å¹¶ä»READMEä¸‹è½½æˆªå›¾"""
    screenshots = []
    
    # è·å–è®ºæ–‡é¡µé¢
    result = subprocess.run(
        ["curl", "-s", f"https://arxiv.org/abs/{arxiv_id}"],
        capture_output=True, text=True, timeout=30
    )
    html = result.stdout
    
    # æŸ¥æ‰¾GitHubé“¾æ¥
    github_match = re.search(r'https://github\.com/[a-zA-Z0-9_-]+/[a-zA-Z0-9_.-]+', html)
    if not github_match:
        return screenshots
    
    github_url = github_match.group(0)
    repo_name = github_url.split('/')[-1]
    
    print(f"    ğŸ” å‘ç°GitHubä»“åº“: {github_url}")
    
    # è·å–README
    readme_result = subprocess.run(
        ["curl", "-s", "-L", f"{github_url}/raw/main/README.md"],
        capture_output=True, text=True, timeout=30
    )
    readme = readme_result.stdout
    
    if not readme or "404" in readme:
        # å°è¯•masteråˆ†æ”¯
        readme_result = subprocess.run(
            ["curl", "-s", "-L", f"{github_url}/raw/master/README.md"],
            capture_output=True, text=True, timeout=30
        )
        readme = readme_result.stdout
    
    if not readme or "404" in readme:
        print(f"    âš ï¸  æ— æ³•è·å–README")
        return screenshots
    
    # ä»READMEæå–å›¾ç‰‡é“¾æ¥ (markdownæ ¼å¼: ![](url) æˆ– <img src="url">)
    img_patterns = [
        r'!\[.*?\]\((https?://[^\)]+\.(?:png|jpg|jpeg|gif))\)',
        r'!\[.*?\]\(([^\)]+\.(?:png|jpg|jpeg|gif))\)',
        r'<img[^>]+src=["\'](https?://[^"\']+\.(?:png|jpg|jpeg|gif))["\']',
        r'<img[^>]+src=["\']([^"\']+\.(?:png|jpg|jpeg|gif))["\']'
    ]
    
    found_images = []
    for pattern in img_patterns:
        matches = re.findall(pattern, readme, re.IGNORECASE)
        for match in matches:
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            if match.startswith('http'):
                found_images.append(match)
            elif match.startswith('./') or match.startswith('../'):
                found_images.append(f"{github_url}/raw/main/{match.lstrip('./')}")
            else:
                found_images.append(f"{github_url}/raw/main/{match}")
    
    # å»é‡å¹¶é™åˆ¶æ•°é‡
    found_images = list(dict.fromkeys(found_images))[:3]  # æœ€å¤š3å¼ 
    
    # ä¸‹è½½å›¾ç‰‡
    for i, img_url in enumerate(found_images):
        img_path = f"$TEMP_DIR/{arxiv_id}_img_{i}.png"
        download_result = subprocess.run(
            ["curl", "-s", "-L", "-o", img_path, img_url],
            capture_output=True, timeout=30
        )
        
        if download_result.returncode == 0 and os.path.exists(img_path):
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆç¡®ä¿ä¸æ˜¯404é¡µé¢ï¼‰
            size = os.path.getsize(img_path)
            if size > 1000:  # è‡³å°‘1KB
                screenshots.append(img_path)
                print(f"    âœ… ä¸‹è½½æˆªå›¾: {img_url[:60]}...")
            else:
                os.remove(img_path)
    
    return screenshots

# åˆ›å»ºæ–‡æ¡£
doc = Document()
doc.styles['Normal'].font.name = 'Microsoft YaHei'
doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), 'Microsoft YaHei')

# æ ‡é¢˜
title = doc.add_heading('AI è®ºæ–‡æ—¥æŠ¥', 0)
title.alignment = WD_ALIGN_PARAGRAPH.CENTER
for run in title.runs:
    run.font.color.rgb = RGBColor(30, 60, 114)

# æ—¥æœŸ
date_para = doc.add_paragraph()
date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
date_run = date_para.add_run(f'ç”Ÿæˆæ—¥æœŸï¼š{datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")}')
date_run.font.size = Pt(12)
date_run.font.color.rgb = RGBColor(100, 100, 100)

doc.add_paragraph()

# å¤„ç†æ¯ç¯‡è®ºæ–‡
papers_list = """$PAPERS""".strip().split()

for idx, paper_id in enumerate(papers_list, 1):
    print(f"[{idx}/{len(papers_list)}] å¤„ç†è®ºæ–‡ {paper_id}...")
    
    # è·å–è®ºæ–‡ä¿¡æ¯
    result = subprocess.run(
        ["curl", "-s", f"https://arxiv.org/abs/{paper_id}"],
        capture_output=True, text=True, timeout=30
    )
    html = result.stdout
    
    # æå–æ ‡é¢˜
    title_match = re.search(r'<h1 class="title mathjax">.*?Title:(.*?)</h1>', html, re.DOTALL)
    paper_title = title_match.group(1).strip() if title_match else f"Paper {paper_id}"
    
    # æå–æ‘˜è¦
    abstract_match = re.search(r'<blockquote class="abstract mathjax">.*?Abstract:(.*?)</blockquote>', html, re.DOTALL)
    abstract = abstract_match.group(1).strip()[:400] if abstract_match else "æš‚æ— æ‘˜è¦"
    
    # æå–ä½œè€…
    authors_match = re.search(r'<div class="authors">(.*?)</div>', html, re.DOTALL)
    authors = "Unknown"
    if authors_match:
        authors_html = authors_match.group(1)
        authors = re.sub(r'<[^>]+>', '', authors_html).replace('Authors:', '').strip()[:100]
    
    # æ·»åŠ æ ‡é¢˜
    doc.add_heading(f"{idx}. {paper_title}", level=2)
    
    # æ·»åŠ ä½œè€…
    if authors != "Unknown":
        p = doc.add_paragraph()
        p.add_run("ğŸ‘¤ ä½œè€…ï¼š").bold = True
        p.add_run(authors)
        p.runs[1].font.size = Pt(10)
        p.runs[1].font.color.rgb = RGBColor(100, 100, 100)
    
    # æ·»åŠ æ‘˜è¦
    p = doc.add_paragraph()
    p.add_run("ğŸ“ æ‘˜è¦ï¼š").bold = True
    p.add_run(abstract)
    p.paragraph_format.line_spacing = 1.5
    
    # æ·»åŠ é“¾æ¥
    p = doc.add_paragraph()
    p.add_run("ğŸ”— é“¾æ¥ï¼š").bold = True
    p.add_run(f"https://arxiv.org/abs/{paper_id}")
    p.runs[1].font.color.rgb = RGBColor(100, 100, 100)
    p.runs[1].font.size = Pt(10)
    
    # å°è¯•è·å–GitHubæˆªå›¾
    try:
        screenshots = fetch_github_screenshots(paper_id)
        if screenshots:
            p = doc.add_paragraph()
            p.add_run("ğŸ“· é¡¹ç›®æˆªå›¾ï¼š").bold = True
            
            for img_path in screenshots:
                try:
                    doc.add_picture(img_path, width=Inches(5.5))
                    last_paragraph = doc.paragraphs[-1]
                    last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
                except Exception as e:
                    print(f"    âš ï¸  æ’å…¥å›¾ç‰‡å¤±è´¥: {e}")
    except Exception as e:
        print(f"    âš ï¸  è·å–æˆªå›¾å¤±è´¥: {e}")
    
    doc.add_paragraph()

# è¶‹åŠ¿æ€»ç»“
doc.add_heading('ğŸ“Š ä»Šæ—¥è¶‹åŠ¿æ€»ç»“', level=1)

summary = """æœ¬æ¬¡æ•´ç†çš„è®ºæ–‡æ¶µç›–äº†ä»¥ä¸‹ä¸»é¢˜æ–¹å‘ï¼š

â€¢ æ¨ç†èƒ½åŠ›ä¸è§„åˆ’ï¼šé•¿ç¨‹è§„åˆ’è¯„ä¼°ã€æ•°å­¦æ¨ç†å¢å¼ºã€æ¦‚ç‡é€»è¾‘æ¨ç†åŠ é€Ÿ
â€¢ æ™ºèƒ½ä½“ä¸é€šä¿¡ï¼šä»»åŠ¡å¯¼å‘é€šä¿¡åè®®ã€å¤šæ¨¡å‹å¯¹è¯å¯¹é½  
â€¢ è®°å¿†ä¸ä¼˜åŒ–ï¼šä¸»åŠ¨å†…å­˜æ§åˆ¶ã€ä¼ä¸šèµ„æºè§„åˆ’
â€¢ æ¶æ„åˆ›æ–°ï¼šæ·±åº¦ç ”ç©¶ç³»ç»Ÿã€ç¥ç»ç¬¦å·AIç¡¬ä»¶åŠ é€Ÿ
â€¢ å®‰å…¨ä¸å¯¹é½ï¼šé£é™©æ•æ„Ÿè§„åˆ’ã€AIå¯¹é½ç­–ç•¥æµ‹è¯•

æ•´ä½“è¶‹åŠ¿æ˜¾ç¤ºï¼ŒAIç ”ç©¶æ­£ä»å•çº¯çš„æ¨¡å‹æ€§èƒ½æå‡è½¬å‘æ›´å¤æ‚çš„ç³»ç»Ÿæ¶æ„è®¾è®¡ã€å¤šæ™ºèƒ½ä½“åä½œä»¥åŠå®‰å…¨å¯¹é½ç­‰æ–¹å‘ã€‚"""

p = doc.add_paragraph(summary)
p.paragraph_format.line_spacing = 1.5

# ä¿å­˜
output_file = "$OUTPUT_DIR/AI_Papers_${DATE_STR}.docx"
doc.save(output_file)
print(f"\nâœ… Wordæ–‡æ¡£å·²ç”Ÿæˆï¼š{output_file}")
PYTHON_EOF

else
    # Markdownæ ¼å¼ï¼ˆç®€ç‰ˆï¼Œä¸åŒ…å«æˆªå›¾ï¼‰
    OUTPUT_FILE="$OUTPUT_DIR/AI_Papers_${DATE_STR}.md"
    
    echo "# AI è®ºæ–‡æ—¥æŠ¥ - ${DATE_STR}" > "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "> è‡ªåŠ¨ç”Ÿæˆçš„ arXiv AI åˆ†ç±»è®ºæ–‡æ‘˜è¦" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    
    INDEX=1
    for PAPER_ID in $PAPERS; do
        echo "[$INDEX] è·å–è®ºæ–‡ $PAPER_ID..."
        PAPER_INFO=$(curl -s "https://arxiv.org/abs/$PAPER_ID")
        TITLE=$(echo "$PAPER_INFO" | grep -o '<h1 class="title mathjax">[^<]*</h1>' | sed 's/<[^>]*>//g' | sed 's/Title://g' | head -1)
        AUTHORS=$(echo "$PAPER_INFO" | grep -o '<div class="authors">.*</div>' | sed 's/<[^>]*>//g' | sed 's/Authors://g' | head -c 100)
        ABSTRACT=$(echo "$PAPER_INFO" | grep -o '<blockquote class="abstract mathjax">.*</blockquote>' | sed 's/<[^>]*>//g' | sed 's/Abstract:[[:space:]]*//g' | head -c 300)
        
        echo "## ${INDEX}. ${TITLE}" >> "$OUTPUT_FILE"
        echo "" >> "$OUTPUT_FILE"
        echo "**ä½œè€…**ï¼š${AUTHORS}..." >> "$OUTPUT_FILE"
        echo "" >> "$OUTPUT_FILE"
        echo "**æ‘˜è¦**ï¼š${ABSTRACT}..." >> "$OUTPUT_FILE"
        echo "" >> "$OUTPUT_FILE"
        echo "**é“¾æ¥**ï¼šhttps://arxiv.org/abs/${PAPER_ID}" >> "$OUTPUT_FILE"
        echo "" >> "$OUTPUT_FILE"
        echo "---" >> "$OUTPUT_FILE"
        echo "" >> "$OUTPUT_FILE"
        
        INDEX=$((INDEX + 1))
        sleep 0.3
    done
    
    echo "âœ… Markdown æ–‡æ¡£å·²ç”Ÿæˆï¼š$OUTPUT_FILE"
fi

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -rf "$TEMP_DIR"
