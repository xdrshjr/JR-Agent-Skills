#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾ç‰‡æœç´¢å’Œä¸‹è½½å·¥å…· - ä¿®å¤ç‰ˆ (v2.0)
æ”¯æŒå¤šä¸ªå›¾ç‰‡æ¥æºï¼Œå¸¦å¤±è´¥å›é€€

ä¿®å¤å†…å®¹:
- ä¿®å¤Wikimedia APIè°ƒç”¨ï¼Œä½¿ç”¨æ­£ç¡®çš„imageinfoæ¥å£è·å–å®é™…å›¾ç‰‡URL
- ç§»é™¤å·²åœç”¨çš„Unsplash Source API
- æ–°å¢Bingå›¾ç‰‡æœç´¢ä½œä¸ºå¤‡é€‰æ¥æº
- æ–°å¢ç›´æ¥URLæºï¼ˆé¢„è®¾å¸¸è§å›¾ç‰‡çš„ç›´æ¥é“¾æ¥ï¼‰
- æ”¹è¿›é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
"""

import argparse
import json
import urllib.request
import urllib.parse
import ssl
import re
from pathlib import Path
from PIL import Image
import time

# ç¦ç”¨SSLéªŒè¯ï¼ˆæŸäº›ç¯å¢ƒä¸‹éœ€è¦ï¼‰
ssl._create_default_https_context = ssl._create_unverified_context

class ImageDownloader:
    """å›¾ç‰‡ä¸‹è½½å™¨ - ä¿®å¤ç‰ˆ"""
    
    def __init__(self, output_dir="images"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        }
    
    def download_image(self, url, filename, timeout=20):
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        output_path = self.output_dir / filename
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if output_path.exists():
            print(f"   âœ… æ–‡ä»¶å·²å­˜åœ¨: {filename}")
            return True
        
        try:
            req = urllib.request.Request(url, headers=self.headers)
            with urllib.request.urlopen(req, timeout=timeout) as response:
                data = response.read()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆå›¾ç‰‡
                if len(data) < 2000:
                    print(f"   âš ï¸  æ–‡ä»¶å¤ªå°: {len(data)} bytes")
                    return False
                
                # ä¿å­˜
                output_path.write_bytes(data)
                
                # éªŒè¯å›¾ç‰‡
                try:
                    with Image.open(output_path) as img:
                        img.verify()
                    size_kb = len(data) / 1024
                    print(f"   âœ… æˆåŠŸä¸‹è½½ ({size_kb:.1f} KB): {filename}")
                    return True
                except Exception as e:
                    print(f"   âš ï¸  æ— æ•ˆå›¾ç‰‡: {e}")
                    output_path.unlink(missing_ok=True)
                    return False
                    
        except Exception as e:
            print(f"   âŒ ä¸‹è½½å¤±è´¥: {str(e)[:50]}")
            return False
    
    def search_wikimedia(self, query, count=3):
        """ä»Wikimedia Commonsæœç´¢å›¾ç‰‡ - ä¿®å¤ç‰ˆ
        
        ä½¿ç”¨æ­£ç¡®çš„APIæµç¨‹:
        1. ä½¿ç”¨search APIæœç´¢æ–‡ä»¶
        2. ä½¿ç”¨imageinfo APIè·å–å®é™…å›¾ç‰‡URL
        3. ä¸‹è½½å›¾ç‰‡
        """
        print(f"ğŸ” Wikimediaæœç´¢: {query}")
        
        # Wikimediaæœç´¢API
        api_url = f"https://commons.wikimedia.org/w/api.php?action=query&list=search&srsearch={urllib.parse.quote(query)}&srnamespace=6&format=json&srlimit={count*2}"
        
        try:
            req = urllib.request.Request(api_url, headers=self.headers)
            with urllib.request.urlopen(req, timeout=15) as response:
                data = json.loads(response.read().decode('utf-8'))
                
                results = []
                for item in data.get('query', {}).get('search', []):
                    title = item.get('title', '').replace('File:', '')
                    if not title:
                        continue
                    
                    # è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬å®é™…URLï¼‰
                    file_info_url = f"https://commons.wikimedia.org/w/api.php?action=query&titles=File:{urllib.parse.quote(title)}&prop=imageinfo&iiprop=url|size|mime&format=json"
                    
                    try:
                        req2 = urllib.request.Request(file_info_url, headers=self.headers)
                        with urllib.request.urlopen(req2, timeout=15) as response2:
                            data2 = json.loads(response2.read().decode('utf-8'))
                            pages = data2.get('query', {}).get('pages', {})
                            
                            for page_id, page_info in pages.items():
                                imageinfo = page_info.get('imageinfo', [])
                                if imageinfo:
                                    img_url = imageinfo[0].get('url')
                                    if img_url:
                                        print(f"   æ‰¾åˆ°: {title[:50]}")
                                        # ä½¿ç”¨æŸ¥è¯¢å…³é”®è¯ä½œä¸ºæ–‡ä»¶åå‰ç¼€
                                        safe_query = query.replace(' ', '_')[:30]
                                        safe_name = f"{safe_query}_{len(results)}.jpg"
                                        
                                        if self.download_image(img_url, safe_name):
                                            results.append(img_url)
                                        
                                        if len(results) >= count:
                                            return results
                                        time.sleep(0.5)
                    except Exception as e:
                        print(f"   è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")
                        continue
                
                return results
                
        except Exception as e:
            print(f"   âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def search_bing_images(self, query, count=3):
        """ä½¿ç”¨Bingå›¾ç‰‡æœç´¢ä½œä¸ºå¤‡é€‰æ¥æº"""
        print(f"ğŸ” Bingå›¾ç‰‡æœç´¢: {query}")
        
        try:
            # Bingå›¾ç‰‡æœç´¢
            search_url = f"https://www.bing.com/images/search?q={urllib.parse.quote(query)}&form=HDRSC2&first=1"
            req = urllib.request.Request(search_url, headers=self.headers)
            
            with urllib.request.urlopen(req, timeout=15) as response:
                html = response.read().decode('utf-8')
                
                # æå–å›¾ç‰‡URLï¼ˆBingå›¾ç‰‡é€šå¸¸åœ¨JSONæ•°æ®æˆ–ç‰¹å®šHTMLç»“æ„ä¸­ï¼‰
                img_urls = []
                
                # å°è¯•å¤šç§æ¨¡å¼åŒ¹é…
                patterns = [
                    r'murl":"(https://[^"]+\.(?:jpg|jpeg|png))"',  # ç›´æ¥å›¾ç‰‡URL
                    r'"ou":"(https://[^"]+\.(?:jpg|jpeg|png))"',  # åŸå§‹URL
                    r'https://tse\d+\.mm\.bing\.net/th\?id=[^\s"<>]+',  # Bingç¼©ç•¥å›¾
                ]
                
                for pattern in patterns:
                    matches = re.findall(pattern, html, re.IGNORECASE)
                    for url in matches:
                        if url not in img_urls and len(img_urls) < count * 3:
                            img_urls.append(url)
                
                results = []
                for i, img_url in enumerate(img_urls[:count]):
                    safe_query = query.replace(' ', '_')[:30]
                    safe_name = f"{safe_query}_bing_{i}.jpg"
                    if self.download_image(img_url, safe_name):
                        results.append(img_url)
                        if len(results) >= count:
                            break
                    time.sleep(0.5)
                
                return results
                
        except Exception as e:
            print(f"   âŒ Bingæœç´¢å¤±è´¥: {e}")
            return []
    
    def search_direct_urls(self, query, count=3):
        """ä½¿ç”¨é¢„è®¾çš„ç›´æ¥URLï¼ˆé’ˆå¯¹å¸¸è§ä¸»é¢˜ï¼‰
        
        ä¼˜ç‚¹ï¼šç»•è¿‡æœç´¢APIï¼Œç›´æ¥ä½¿ç”¨å·²çŸ¥çš„æœ‰æ•ˆå›¾ç‰‡é“¾æ¥
        é€‚ç”¨äºï¼šè‰ºæœ¯å²å¸¸è§ä¸»é¢˜ï¼ˆå¡”ç‰¹æ—å¡”ã€æ„æˆä¸»ä¹‰ä½œå“ç­‰ï¼‰
        """
        print(f"ğŸ” ç›´æ¥URLæœç´¢: {query}")
        
        # é¢„è®¾å¸¸è§å›¾ç‰‡çš„ç›´æ¥URL
        direct_urls = {
            "tatlin tower": [
                "https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Tatlin_Tower.jpg/800px-Tatlin_Tower.jpg",
                "https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Monument_to_the_Third_International.jpg/800px-Monument_to_the_Third_International.jpg",
            ],
            "constructivism": [
                "https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/Rodchenko_Constructivist.jpg/800px-Rodchenko_Constructivist.jpg",
            ],
            "rodchenko": [
                "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Alexander_Rodchenko.jpg/800px-Alexander_Rodchenko.jpg",
            ],
            "lissitzky": [
                "https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/El_Lissitzky.jpg/800px-El_Lissitzky.jpg",
                "https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Lissitzky_Proun.jpg/800px-Lissitzky_Proun.jpg",
            ],
            "material design": [
                "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Material_Design.svg/1024px-Material_Design.svg.png",
            ],
        }
        
        results = []
        query_lower = query.lower()
        
        for key, urls in direct_urls.items():
            if key in query_lower:
                for i, url in enumerate(urls[:count]):
                    safe_query = query.replace(' ', '_')[:30]
                    safe_name = f"{safe_query}_direct_{i}.jpg"
                    if self.download_image(url, safe_name):
                        results.append(url)
                        if len(results) >= count:
                            break
                    time.sleep(0.5)
                break
        
        return results
    
    def search_with_retry(self, query, sources=None, max_per_source=2):
        """
        å¤šæºæœç´¢å¸¦é‡è¯•
        
        å‚æ•°:
            query: æœç´¢å…³é”®è¯
            sources: å›¾ç‰‡æ¥æºåˆ—è¡¨ ['direct', 'wikimedia', 'bing', ...]
            max_per_source: æ¯ä¸ªæ¥æºæœ€å¤§ä¸‹è½½æ•°
        
        é»˜è®¤æœç´¢é¡ºåºï¼šdirect -> wikimedia -> bing
        """
        if sources is None:
            sources = ['direct', 'wikimedia', 'bing']
        
        all_results = []
        
        for source in sources:
            print(f"\nğŸ“¡ å°è¯•æ¥æº: {source}")
            
            if source == 'direct':
                results = self.search_direct_urls(query, max_per_source)
            elif source == 'wikimedia':
                results = self.search_wikimedia(query, max_per_source)
            elif source == 'bing':
                results = self.search_bing_images(query, max_per_source)
            else:
                print(f"   æœªçŸ¥æ¥æº: {source}")
                continue
            
            all_results.extend(results)
            
            if len(all_results) >= max_per_source:
                print(f"âœ… å·²ä¸‹è½½è¶³å¤Ÿå›¾ç‰‡ ({len(all_results)}å¼ )")
                break
            
            time.sleep(1)  # æ¥æºé—´å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        
        return all_results


def create_placeholder_image(name, title, output_dir, size=(800, 600)):
    """åˆ›å»ºå ä½å›¾ï¼ˆå½“ç½‘ç»œä¸‹è½½å¤±è´¥æ—¶ï¼‰"""
    from PIL import ImageDraw, ImageFont
    
    output_path = Path(output_dir) / f"{name}.jpg"
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
    if output_path.exists():
        print(f"   âœ… å ä½å›¾å·²å­˜åœ¨: {output_path}")
        return output_path
    
    # åˆ›å»ºå›¾åƒ
    img = Image.new('RGB', size, (240, 240, 240))
    draw = ImageDraw.Draw(img)
    
    # å°è¯•è·å–å­—ä½“
    try:
        font_title = ImageFont.truetype("/System/Library/Fonts/PingFang.ttc", 36)
        font_note = ImageFont.truetype("/System/Library/Fonts/PingFang.ttc", 18)
    except:
        font_title = ImageFont.load_default()
        font_note = font_title
    
    # ç»˜åˆ¶è£…é¥°ï¼ˆæ„æˆä¸»ä¹‰é£æ ¼ï¼‰
    draw.rectangle([(40, 100), (60, size[1]-100)], fill=(200, 50, 50))
    draw.rectangle([(40, size[1]-80), (300, size[1]-60)], fill=(200, 50, 50))
    
    # ç»˜åˆ¶æ–‡å­—
    draw.text((100, size[1]//2 - 30), title, fill=(100, 100, 100), font=font_title)
    draw.text((100, size[1] - 50), f"[{name}]", fill=(180, 180, 180), font=font_note)
    
    img.save(output_path, 'JPEG', quality=90)
    print(f"   âœ… å ä½å›¾å·²åˆ›å»º: {output_path}")
    return output_path


def main():
    parser = argparse.ArgumentParser(description="å›¾ç‰‡æœç´¢ä¸‹è½½å·¥å…· - ä¿®å¤ç‰ˆ")
    parser.add_argument("--query", "-q", required=True, help="æœç´¢å…³é”®è¯")
    parser.add_argument("--output", "-o", default="images", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--count", "-n", type=int, default=3, help="ä¸‹è½½æ•°é‡")
    parser.add_argument("--sources", "-s", nargs='+', 
                       default=['direct', 'wikimedia', 'bing'],
                       help="å›¾ç‰‡æ¥æº: direct(ç›´æ¥URL), wikimedia, bing")
    parser.add_argument("--placeholder", "-p", action="store_true",
                       help="å¦‚æœä¸‹è½½å¤±è´¥åˆ™åˆ›å»ºå ä½å›¾")
    parser.add_argument("--name", default="image", help="å ä½å›¾æ–‡ä»¶å")
    
    args = parser.parse_args()
    
    print("="*60)
    print("å›¾ç‰‡æœç´¢ä¸‹è½½å·¥å…· - ä¿®å¤ç‰ˆ")
    print("="*60)
    print(f"æœç´¢: {args.query}")
    print(f"æ¥æº: {', '.join(args.sources)}")
    print(f"æ•°é‡: {args.count}")
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = ImageDownloader(output_dir=args.output)
    
    # æœç´¢ä¸‹è½½
    results = downloader.search_with_retry(
        args.query,
        sources=args.sources,
        max_per_source=args.count
    )
    
    print("\n" + "="*60)
    print(f"ğŸ“Š ç»“æœ: æˆåŠŸä¸‹è½½ {len(results)} å¼ å›¾ç‰‡")
    
    # å¦‚æœå¤±è´¥ä¸”å¯ç”¨å ä½å›¾
    if len(results) == 0 and args.placeholder:
        print("\nğŸ“ åˆ›å»ºå ä½å›¾...")
        create_placeholder_image(args.name, args.query, args.output)
    
    print("="*60)


if __name__ == "__main__":
    main()
